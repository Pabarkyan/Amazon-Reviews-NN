{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pickle\n",
    "from tensorflow.keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reseñas</th>\n",
       "      <th>IA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>longitud_promedio</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>signos_de_puntuación</th>\n",
       "      <th>frecuencia_pronombres</th>\n",
       "      <th>variedad_lexica</th>\n",
       "      <th>entropia_lexica</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>palabras_complejas</th>\n",
       "      <th>tecnicismos</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>subjetividad</th>\n",
       "      <th>coherencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Necesitaba un nuevo movil y me decidí por este...</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>5.267482</td>\n",
       "      <td>5.640761e-02</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está en perfectas condiciones la batería al 10...</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Me llegó ayer el iPhone en perfectas condicion...</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>4.402439</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>5.710972</td>\n",
       "      <td>5.749596e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compré  un iPhone 12 a este vendedor y hace un...</td>\n",
       "      <td>0</td>\n",
       "      <td>657</td>\n",
       "      <td>4.124031</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>5.880116</td>\n",
       "      <td>7.866846e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soy usuario de IPhone desde el principio. Teng...</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>4.109244</td>\n",
       "      <td>0.638655</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>5.953860</td>\n",
       "      <td>7.357145e-02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reseñas  IA  longitud  \\\n",
       "0  Necesitaba un nuevo movil y me decidí por este...   0       297   \n",
       "1  Está en perfectas condiciones la batería al 10...   0        99   \n",
       "2  Me llegó ayer el iPhone en perfectas condicion...   0       436   \n",
       "3  Compré  un iPhone 12 a este vendedor y hace un...   0       657   \n",
       "4  Soy usuario de IPhone desde el principio. Teng...   0       596   \n",
       "\n",
       "   longitud_promedio  palabras_unicas  signos_de_puntuación  \\\n",
       "0           4.066667         0.733333                     6   \n",
       "1           5.375000         1.000000                     2   \n",
       "2           4.402439         0.695122                     7   \n",
       "3           4.124031         0.581395                    10   \n",
       "4           4.109244         0.638655                    11   \n",
       "\n",
       "   frecuencia_pronombres  variedad_lexica  entropia_lexica         tfidf  \\\n",
       "0                      0         0.733333         5.267482  5.640761e-02   \n",
       "1                      0         1.000000         4.000000  5.551115e-17   \n",
       "2                      0         0.707317         5.710972  5.749596e-02   \n",
       "3                      1         0.604651         5.880116  7.866846e-02   \n",
       "4                      0         0.655462         5.953860  7.357145e-02   \n",
       "\n",
       "   palabras_complejas  tecnicismos  polaridad  subjetividad  coherencia  \n",
       "0                   3            0        0.0           0.0    0.920000  \n",
       "1                   2            0        0.0           0.0    1.000000  \n",
       "2                   5            0        0.0           0.0    0.857143  \n",
       "3                   5            0        0.0           0.0    0.823529  \n",
       "4                   4            0        0.0           0.0    0.953488  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el csv\n",
    "FICHERO_DATA = 'data/data_reviews.csv'\n",
    "df = pd.read_csv(FICHERO_DATA)\n",
    "df.drop(columns=['expresiones'], inplace=True) # No aporta nada y el futuro df de validación no tendrá esta columna\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reseñas', 'IA', 'longitud', 'longitud_promedio', 'palabras_unicas',\n",
       "       'signos_de_puntuación', 'frecuencia_pronombres', 'variedad_lexica',\n",
       "       'entropia_lexica', 'tfidf', 'palabras_complejas', 'tecnicismos',\n",
       "       'polaridad', 'subjetividad', 'coherencia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pabma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 94ms/step - accuracy: 0.5224 - loss: 0.9466 - val_accuracy: 0.7397 - val_loss: 0.6096\n",
      "Epoch 2/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.6586 - loss: 0.7033 - val_accuracy: 0.8048 - val_loss: 0.5430\n",
      "Epoch 3/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.6774 - loss: 0.6590 - val_accuracy: 0.8219 - val_loss: 0.4725\n",
      "Epoch 4/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 94ms/step - accuracy: 0.7111 - loss: 0.6322 - val_accuracy: 0.8185 - val_loss: 0.4479\n",
      "Epoch 5/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 95ms/step - accuracy: 0.7208 - loss: 0.6040 - val_accuracy: 0.8322 - val_loss: 0.4352\n",
      "Epoch 6/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.7151 - loss: 0.5999 - val_accuracy: 0.8425 - val_loss: 0.4277\n",
      "Epoch 7/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7468 - loss: 0.5838 - val_accuracy: 0.8322 - val_loss: 0.4228\n",
      "Epoch 8/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.7479 - loss: 0.5411 - val_accuracy: 0.8322 - val_loss: 0.4197\n",
      "Epoch 9/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.7530 - loss: 0.5496 - val_accuracy: 0.8288 - val_loss: 0.4206\n",
      "Epoch 10/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.7410 - loss: 0.5433 - val_accuracy: 0.8322 - val_loss: 0.4189\n",
      "Epoch 11/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.7571 - loss: 0.5252 - val_accuracy: 0.8253 - val_loss: 0.4169\n",
      "Epoch 12/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - accuracy: 0.7468 - loss: 0.5545 - val_accuracy: 0.8322 - val_loss: 0.4143\n",
      "Epoch 13/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 90ms/step - accuracy: 0.7539 - loss: 0.5319 - val_accuracy: 0.8390 - val_loss: 0.4150\n",
      "Epoch 14/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.7749 - loss: 0.5113 - val_accuracy: 0.8390 - val_loss: 0.4122\n",
      "Epoch 15/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - accuracy: 0.7769 - loss: 0.5141 - val_accuracy: 0.8322 - val_loss: 0.4125\n",
      "Epoch 16/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - accuracy: 0.7541 - loss: 0.5205 - val_accuracy: 0.8288 - val_loss: 0.4146\n",
      "Epoch 17/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 0.7813 - loss: 0.5034 - val_accuracy: 0.8219 - val_loss: 0.4174\n",
      "Epoch 18/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 0.7716 - loss: 0.5015 - val_accuracy: 0.8219 - val_loss: 0.4165\n",
      "Epoch 19/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 94ms/step - accuracy: 0.7727 - loss: 0.4901 - val_accuracy: 0.8356 - val_loss: 0.4122\n",
      "Epoch 20/20\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.7798 - loss: 0.4822 - val_accuracy: 0.8425 - val_loss: 0.4126\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7595 - loss: 0.4589\n",
      "Accuracy: 0.783561646938324\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal hibrida ya que analiza caracteristicas numericas obtenidas previamente con procesamiento del lenguaje natural y caracteristicas en texto\n",
    "# mediante un Embedding y una capa LSTM para capturar relaciones temporales en el texto.\n",
    "\n",
    "# 1. Separar características numéricas y texto\n",
    "X_text = df['reseñas']\n",
    "X_numerical = df.drop(columns=['reseñas', 'IA'])\n",
    "y = df['IA']\n",
    "\n",
    "# 2. Separación de los conjuntos de entrenamiento y prueba\n",
    "X_train_text, X_test_text, X_train_numerical, X_test_numerical, y_train, y_test = train_test_split(\n",
    "    X_text, X_numerical, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Escalado de las características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_scaled = scaler.fit_transform(X_train_numerical)\n",
    "X_test_numerical_scaled = scaler.transform(X_test_numerical)\n",
    "\n",
    "# 4. Tokenización del texto (reseñas)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['reseñas'])\n",
    "\n",
    "# Tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Añadimos 1 porque el indexado empieza en 1\n",
    "max_length = X_text.apply(lambda x: len(x.split())).max()  # Longitud máxima de las secuencias\n",
    "\n",
    "# Convertir texto en secuencias de enteros\n",
    "X_train_text_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_text_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Padding de las secuencias para que todas tengan la misma longitud\n",
    "X_train_text_padded = pad_sequences(X_train_text_seq, maxlen=max_length, padding='post')\n",
    "X_test_text_padded = pad_sequences(X_test_text_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# 5. Definir el modelo más complejo\n",
    "numerical_input = Input(shape=(X_train_numerical_scaled.shape[1],), name='numerical_input')\n",
    "x1 = Dense(128, activation='relu')(numerical_input)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "text_input = Input(shape=(max_length,), name='text_input')\n",
    "x2 = Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length)(text_input)\n",
    "x2 = LSTM(128, return_sequences=False)(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "combined = Concatenate()([x1, x2])\n",
    "\n",
    "x = Dense(64, activation='relu')(combined)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[numerical_input, text_input], outputs=output)\n",
    "\n",
    "# 6. Compilar el modelo\n",
    "model.compile(optimizer=Adam(learning_rate=5e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 7. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    [X_train_numerical_scaled, X_train_text_padded],\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, accuracy = model.evaluate([X_test_numerical_scaled, X_test_text_padded], y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Guardar el modelo en formato HDF5\n",
    "model.save('data/modelo_LSTM.h5')\n",
    "\n",
    "# Guardar los datos preprocesados en un archivo pickle\n",
    "with open('data/datos_preprocesados.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'modelo_LSTM': model,\n",
    "        'X_test_numerical_scaled': X_test_numerical_scaled,\n",
    "        'X_test_text_padded': X_test_text_padded,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'max_length': max_length,\n",
    "        'tokenizer': tokenizer\n",
    "    }, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
